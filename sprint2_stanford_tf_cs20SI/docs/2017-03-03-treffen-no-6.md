# 6. Treffen Tensorflow-Lerngruppe

Datum: 03.03.2017
Ort: Pioniergarage

## Agenda:

- Weitermachen mit dem Stanford Kurs: [CS 20SI: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/index.html), [1. Assignment](http://web.stanford.edu/class/cs20si/assignments/a1.pdf) soll das nächste Mal besprochen werden


## Protokoll
Main Takeaways der Lecture Notes:

### 1. Welcome to TensorFlow 
Ein TF Model ist in 2 Phasen unterteilt
- Phase 1: assemble a graph
- Phase 2: use a session to execute operations in the graph
- A Tensor is an n-dimensional matrix
  - 0-d tensor: scalar (number)
  - 1-d tensor: vector
  - 2-d tensor: matrix

What if I want to build more than one graph? -> Benutze nicht verschiedene Graphen, sondern einen großen Graph,
der aus Subgraphen besteht, welche nicht miteinander verbunden sind: "It’s better to have disconnected subgraphs within one graph"

### 2. TensorFlow Ops

#### Fun with Tensorboard
- Visualisierung von Graphen auf TensorBoard durch das Schreiben von Summaries bei der Graphenerstellung: 
`writer = tf.summary.FileWriter(logs_dir,sess.graph)`
- Aufrufen von Tensorboard mit `tensorboard --logdir="./graphs"`
- Man kann sich auch das serialisierte Format des Graphen anschauen. Der Graph wird mit Protobuf serialisiert und anschließend von Tensorboard interpretiert. Folgender Befehl gibt einen Output in JSON:
```
import tensorflow as tf

my_const = tf.constant([1.0, 2.0], name="my_const")
print tf.get_default_graph().as_graph_def()
```

#### Constant Types
- Definition von Konstanten (immutable) mit ihrerm jeweiligen Datentyp (_dtype_) & der Dimension (_shape_): 
  `tf.zeros(shape,   dtype=tf.float32, name=None)`
- Es können auch Konstanten aus Verteilungen gesampled werden
> Note that unlike NumPy or Python sequences, TensorFlow sequences are not iterable.

#### Math Ops
- Mathematische Operationen: pretty standard
- In den Lecture Notes / Slides gibt es eine gute Übersichtsgrafik dazu: Element-Wise-, Array-, Matrix-Ops, Neural Network 
  Building Blocks, ...

#### Data Types 
> TL;DR: Most of the times, you can use TensorFlow types and NumPy types interchangeably
- Im Gegensatz zu Numpy kann man die Präzision der Datatypes ändern -> besseres Memory Management, z.B. Int8-64, Float32-64, ...

#### Variables
> 1. A constant is constant. A variable can be assigned to, its value can be changed.
> 2. A constant's value is stored in the graph and its value is replicated wherever the graph is
> loaded. A variable is stored separately, and may live on a parameter server.

- Variablen werden in der Graphendefinition deklariert und müssen infolge initialisiert werden:
```
init = tf.global_variables_initializer()
with tf .Session() as sess:
sess.run(init)
```

> If we print the initialized variable, we only see the tensor object.

Also muss die Variable vor dem Printen evaluiert werden mit `tf.Variable.eval()`

> You can, of course, declare a variable that depends on other variables.

#### Interactive Session
> You sometimes see InteractiveSession instead of Session. The only difference is an
> InteractiveSession makes itself the default session so you can call run() or eval() without
> explicitly call the session. This is convenient in interactive shells and IPython notebooks

#### Control Dependencies
```
# your graph g have 5 ops: a, b, c, d, e
with g.control_dependencies ([ a , b ,c]):
# `d` and `e` will only run after `a`, `b`, and `c` have executed.
d = ...
e = ...
```

#### Placeholders and feed_dict
Placeholder sind "Platzhalter" für den Input in unseren Graphen (z.B. MNIST-Bilder, Datenpunkte). Placeholders können Restriktionen setzen wie z.B. _shape_ und _dtype_.`tf.placeholder(dtype, shape=None, name=None)`
> Shape specifies the shape of the tensor that can be accepted as actual value for the
> placeholder. shape=None means that tensors of any shape will be accepted. Using
> shape=None is easy to construct graphs, but nightmarish for debugging. You should always
> define the shape of your placeholders as detailed as possible.


> We can feed as any data points to the placeholder as we want by iterating through the data set
and feed in the value one at a time
```
with tf.Session() as sess:
for a_value in list_of_a_values:
print(sess.run(c, { a :a_value}))
```

#### The Trap of Lazy Loading 
- Bitte in den Lecture Notes nachschauen. Es ist z.B. relevant bei der Wiederverwendbarkeit von Code und Darstellung von Zwischenergebnissen.

### Todos:

- Assignment 1 fertig bearbeiten [CS 20SI: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/index.html), 
[1. Assignment](http://web.stanford.edu/class/cs20si/assignments/a1.pdf) soll das nächste Mal besprochen werden


#### Nächstes Treffen: 10. März 2017, 16:00 - 18:00, Pioniergarage
